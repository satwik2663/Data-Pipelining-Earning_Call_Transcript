{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getiing the list of companies in DOW_30\n",
    "def dow_30_companies_func():\n",
    "    url = 'https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average'\n",
    "    response = requests.get(url)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    table = soup.findChildren('table')[1]\n",
    "    rows = table.find_all('tr')\n",
    "    all_cols = []\n",
    "    for row in rows:\n",
    "        cols=row.find_all('td')\n",
    "        cols=[x.text.strip() for x in cols]\n",
    "        all_cols.append(cols)\n",
    "    doq_30_df = pd.DataFrame(all_cols, columns=['Company', 'Echange','Symbol','Industry','Date_Added','Notes'])\n",
    "    doq_30_df.drop(doq_30_df.index[[0]], inplace = True)\n",
    "    doq_30_df['Symbol'] = doq_30_df['Symbol'].str.replace('NYSE:', ' ')\n",
    "    print('Dow 30 Comapnies have been scraped')\n",
    "    return doq_30_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ear_call_trans(i,cookies):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36',\n",
    "               'cookie': cookies}\n",
    "    temp_list = []\n",
    "    url = 'https://seekingalpha.com/symbol/'+str(i).strip() + '/earnings/transcripts'\n",
    "    #print(url)\n",
    "    response = requests.get(url,headers=headers)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    for a in soup.find_all('a', string=re.compile('Earnings Call Transcript')):\n",
    "        temp_list.append('https://seekingalpha.com' + a['href'])\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(doq_30_df,cookies):\n",
    "    list_all = []\n",
    "    for i in doq_30_df['Symbol'].tolist():\n",
    "        res = ear_call_trans(i,cookies)\n",
    "        list_all.append(res)\n",
    "    df = pd.DataFrame(list_all)\n",
    "    df = df.T\n",
    "    df.columns= doq_30_df['Company'].tolist()[:]\n",
    "    df = df.fillna('EMPTY')\n",
    "    print('Companies Earning calls links have been scraped')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scarping the URL of each company to get the speaker and the paragraph\n",
    "\n",
    "def get_each_link(link, cookies):\n",
    "    str_para = ''\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36',\n",
    "           'cookie': cookies}\n",
    "    response = requests.get(link, headers=headers)\n",
    "    data = response.text\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    para = soup.find_all('p')\n",
    "    return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qtr_year(link):\n",
    "    qtr_year = []\n",
    "    qtr_year.append(link[-40:-38])\n",
    "    qtr_year.append(link[-37:-33])\n",
    "    return qtr_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_companies(df, cookies):\n",
    "    list_a = []\n",
    "    for col in df.columns:\n",
    "        values = df[col].tolist()\n",
    "        values[:] = [x for x in values if 'EMPTY' not in x]\n",
    "        for link in values:\n",
    "            qtr_year = get_qtr_year(link)\n",
    "            qtr = qtr_year[0]\n",
    "            year = qtr_year[1]\n",
    "            paragraphs  = get_each_link(link, cookies)\n",
    "            list_a.append([col,link,qtr,year,paragraphs]) \n",
    "            time.sleep(5)\n",
    "    df_store = pd.DataFrame(list_a, columns = ['company','link', 'qtr','year','text']) \n",
    "    print('Companies Earning calls text have been scraped')\n",
    "    return df_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun_companies(df, cookies):\n",
    "    list_a = []\n",
    "    pattern = r'[^.]*please\\ enable\\ Javascript\\ and\\ cookies\\ in\\ your\\ browser'\n",
    "    rerun_list = df[df['text'].str.match(pattern)]['link'].tolist()\n",
    "    rerun_company_list =  df[df['text'].str.match(pattern)]['company'].tolist()\n",
    "    for link in rerun_list:\n",
    "        print(link)\n",
    "        qtr_year = get_qtr_year(link)\n",
    "        qtr = qtr_year[0]\n",
    "        year = qtr_year[1]\n",
    "        paragraphs  = get_each_link(link, cookies)\n",
    "        list_a.append([link,qtr,year,paragraphs]) \n",
    "        time.sleep(5)\n",
    "    df_temp = pd.DataFrame(list_a, columns = ['link', 'qtr','year','text']) \n",
    "    df_temp['company'] = rerun_company_list                                              \n",
    "    df = df[~df['link'].isin(rerun_list)]\n",
    "    df.append(df_temp)\n",
    "    rerun_list = df[df['text'].str.match(pattern)]['link'].tolist()\n",
    "    df = df[~df['link'].isin(rerun_list)]\n",
    "    print('Failed to scrape companies have been scraped again')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mappings(soup):\n",
    "    mappings = []\n",
    "    heading_on = False\n",
    "    all_text = soup.find_all('p')\n",
    "    heads_counter = 0\n",
    "    heads_list = soup.find_all('strong')\n",
    "    for i in range(len(all_text)):\n",
    "        heads = all_text[i].find_all('strong')\n",
    "        if len(heads)>0 and not heading_on:\n",
    "            pair_id =  heads_list[heads_counter].text\n",
    "            for each in all_text[i].find_all_next(\"p\"):\n",
    "                #pair_id =  each.text\n",
    "                if len(each.find_all('strong')) == 0:\n",
    "                    #print(pair_id+\"-\"+each.text)\n",
    "                    mappings.append((pair_id,each.text))\n",
    "                else:\n",
    "                    heads_counter = heads_counter+1\n",
    "                    ##print(each)\n",
    "                    if len(each.find_all('span'))>0 and each.find_all('span')[0]['class'][0]=='question':\n",
    "                        print('question')\n",
    "                        mappings.append((heads_list[heads_counter].text,'question',each.text))\n",
    "                    if len(each.find_all('span'))>0 and each.find_all('span')[0]['class'][0]=='answer':\n",
    "                        print('answer')\n",
    "                        mappings.append((heads_list[heads_counter].text,'answer',each.text))\n",
    "                    break\n",
    "    processed_intro_mappings = []\n",
    "    q_and_a = False\n",
    "    for each in range(len(mappings)):\n",
    "        if len(mappings[each])==2 and not q_and_a:\n",
    "            q_and_a = False\n",
    "            processed_intro_mappings.append(mappings[each])\n",
    "        if len(mappings[each])==3:\n",
    "            q_and_a = True\n",
    "            break\n",
    "    processed_qa_mappings = []\n",
    "    q_and_a = False\n",
    "    for each in range(len(mappings)):\n",
    "        if len(mappings[each])==2 and q_and_a:\n",
    "            q_and_a = False\n",
    "            #processed_qa_mappings.append(mappings[each])\n",
    "        if len(mappings[each])==3:\n",
    "            q_and_a = True\n",
    "            processed_qa_mappings.append((mappings[each][0],mappings[each][1],mappings[each+1][1]))\n",
    "            continue\n",
    "    processed_conclusion_mappings = []\n",
    "    q_and_a_start = False\n",
    "    q_and_a_over =  False\n",
    "    for each in reversed(range(len(mappings))):\n",
    "        if len(mappings[each])==2 and not q_and_a_over:\n",
    "            q_and_a_over = False\n",
    "            processed_conclusion_mappings.append(mappings[each])\n",
    "        if len(mappings[each])==3:\n",
    "            q_and_a_over = True\n",
    "            break\n",
    "    processed_conclusion_mappings.reverse()\n",
    "    \n",
    "    return [processed_intro_mappings, processed_qa_mappings, processed_conclusion_mappings]\n",
    "    \n",
    "    list_all = []\n",
    "for each in data.loc[:,'text']:\n",
    "    soup = BeautifulSoup(each, 'html.parser')\n",
    "    list_all.append(get_mappings(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dow 30 Comapnies have been scraped\n",
      "Enter your cookies: machine_cookie=7952404426213; _gcl_au=1.1.1852272509.1569899012; _ga=GA1.2.1953747303.1569899012; _pxvid=0b7e782c-e3f8-11e9-b462-0242ac12000d; _fbp=fb.1.1569899012051.886186956; __adroll_fpc=aae1eb2e906a50f3949b56c1d8d42366-s2-1569899012247; h_px=1; __gads=ID=d63a506c6419e1b6:T=1569899012:S=ALNI_Ma0SgjcuKl1JA1c7RW99nLBYW7VRw; _gcl_aw=GCL.1570401017.Cj0KCQjwz8bsBRC6ARIsAEyNnvpAH7BQxZHxIKHWYLIWMAxz1p-SAGiDnoSVERLTrVsJIhr3fxTIqSsaApoAEALw_wcB; _gac_UA-142576245-1=1.1570401017.Cj0KCQjwz8bsBRC6ARIsAEyNnvpAH7BQxZHxIKHWYLIWMAxz1p-SAGiDnoSVERLTrVsJIhr3fxTIqSsaApoAEALw_wcB; _gid=GA1.2.1752854065.1570401017; __aaxsc=0; _pxff_tm=1; user_id=50729313; user_nick=shiqi; user_devices=; u_voc=; marketplace_author_slugs=; user_cookie_key=0; has_paid_subscription=false; user_perm=; sapu=101; user_remember_token=942f39e40b9d0a7ae094f54356c5178db00607bf; free_article=0; url_source_before_register=https%3A%2F%2Fseekingalpha.com%2Fsubscriptions; regsteps=vocation%2Cnewsletters%2Cstocks; _dc_gtm_UA-142576245-1=1; _gat_UA-142576245-1=1; portfolio_sort_type=a_z; __ar_v4=2EEQPRZIBZB7VIPPEX2IGK%3A20191005%3A1%7CULCHBRH4ZZGFXDWGQTC6RG%3A20190931%3A7%7CRFXAEISDJFDZDINVACZG6X%3A20190931%3A15%7CHWYEUMZG3RCB3IJESAMRSO%3A20190931%3A15%7CRQ5QC664UFDO7B7WUQLCWR%3A20190931%3A5%7CDZPINTYKVVC37LE5MJWGEE%3A20191005%3A1%7CF6X65CJ4K5E43AFRH5CGQD%3A20191005%3A1; gk_user_access=1**1570401330; gk_user_access_sign=ab3e4c0a5e8bfeae794ee871e69d658ad6543138; _igt=63111f8a-2cc0-4c8e-ef39-30312ad92a9f; _ig=23c932c9-df53-4b37-92ae-a42d6a8fd7e0; _hjid=1c813c65-9363-4ac1-bebb-f9ec98b3612b; _hjIncludedInSample=1; _px2=eyJ1IjoiOWFlMzZjMzAtZTg4OS0xMWU5LWIyZDYtZDE1MjkzZmI2ZTdiIiwidiI6IjBiN2U3ODJjLWUzZjgtMTFlOS1iNDYyLTAyNDJhYzEyMDAwZCIsInQiOjE1NzA0MDE4MzM0NTYsImgiOiIwYWY1MmNmMjMxMDVjODRiNGYyYmY3MWZhNWNiMTAxYzYwYzQ2MDVlOTQ0ZTI0MTExZDdhNTZjYmNiN2MyNTFlIn0=; _px=2aTrZ4YsWDPHKEWd8SkGSknh+MguaFO5v08Zp5FpuXihOKSBfCNW1kqz0Sh6TeY6yCxZB569T9wcqASHHLqO8w==:1000:ktdy5OvwfDY+9o/aY+fpibJo69IGspZ0rUiGyjqHr6mPvPrMzOeSncOiyE2DwR0n6XM9iFjSH45EQdDEtT7C7jywMISr/wig/QhIjCSIfjb6oN8OD5BSZIdDe9iOqI3pTIMDjAFBA8pN1OG2YClj7veSzw0gcFqQibMkTSUhXrlSWu0rtsixXA5AzmhXTic/oiIgfjYO+ArCArHmirIjyWPoomfbnd9sT/fxhC6Yha6uzK5qIw10BhAAUXUQU8oKhYP/hs+r3ZnuGOoyHPjtmg==; _pxde=f28ee823697b2a8c811f655dd2d1091c8bd93e6933895256adebef386d4e4bd0:eyJ0aW1lc3RhbXAiOjE1NzA0MDEzMzM0NTYsImZfa2IiOjB9; aasd=11%7C1570401017239\n",
      "Companies Earning calls links have been scraped\n",
      "Companies Earning calls text have been scraped\n",
      "Failed to scrape companies have been scraped again\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    dow_30_companies = dow_30_companies_func() \n",
    "    cookies = input(\"Enter your cookies: \")\n",
    "    #shiqi_cookie = 'machine_cookie=7952404426213; _gcl_au=1.1.1852272509.1569899012; _ga=GA1.2.1953747303.1569899012; _pxvid=0b7e782c-e3f8-11e9-b462-0242ac12000d; _fbp=fb.1.1569899012051.886186956; __adroll_fpc=aae1eb2e906a50f3949b56c1d8d42366-s2-1569899012247; h_px=1; __gads=ID=d63a506c6419e1b6:T=1569899012:S=ALNI_Ma0SgjcuKl1JA1c7RW99nLBYW7VRw; _gcl_aw=GCL.1570401017.Cj0KCQjwz8bsBRC6ARIsAEyNnvpAH7BQxZHxIKHWYLIWMAxz1p-SAGiDnoSVERLTrVsJIhr3fxTIqSsaApoAEALw_wcB; _gac_UA-142576245-1=1.1570401017.Cj0KCQjwz8bsBRC6ARIsAEyNnvpAH7BQxZHxIKHWYLIWMAxz1p-SAGiDnoSVERLTrVsJIhr3fxTIqSsaApoAEALw_wcB; _gid=GA1.2.1752854065.1570401017; __aaxsc=0; _pxff_tm=1; user_id=50729313; user_nick=shiqi; user_devices=; u_voc=; marketplace_author_slugs=; user_cookie_key=0; has_paid_subscription=false; user_perm=; sapu=101; user_remember_token=942f39e40b9d0a7ae094f54356c5178db00607bf; free_article=0; url_source_before_register=https%3A%2F%2Fseekingalpha.com%2Fsubscriptions; regsteps=vocation%2Cnewsletters%2Cstocks; _dc_gtm_UA-142576245-1=1; _gat_UA-142576245-1=1; portfolio_sort_type=a_z; __ar_v4=2EEQPRZIBZB7VIPPEX2IGK%3A20191005%3A1%7CULCHBRH4ZZGFXDWGQTC6RG%3A20190931%3A7%7CRFXAEISDJFDZDINVACZG6X%3A20190931%3A15%7CHWYEUMZG3RCB3IJESAMRSO%3A20190931%3A15%7CRQ5QC664UFDO7B7WUQLCWR%3A20190931%3A5%7CDZPINTYKVVC37LE5MJWGEE%3A20191005%3A1%7CF6X65CJ4K5E43AFRH5CGQD%3A20191005%3A1; gk_user_access=1**1570401330; gk_user_access_sign=ab3e4c0a5e8bfeae794ee871e69d658ad6543138; _igt=63111f8a-2cc0-4c8e-ef39-30312ad92a9f; _ig=23c932c9-df53-4b37-92ae-a42d6a8fd7e0; _hjid=1c813c65-9363-4ac1-bebb-f9ec98b3612b; _hjIncludedInSample=1; _px2=eyJ1IjoiOWFlMzZjMzAtZTg4OS0xMWU5LWIyZDYtZDE1MjkzZmI2ZTdiIiwidiI6IjBiN2U3ODJjLWUzZjgtMTFlOS1iNDYyLTAyNDJhYzEyMDAwZCIsInQiOjE1NzA0MDE4MzM0NTYsImgiOiIwYWY1MmNmMjMxMDVjODRiNGYyYmY3MWZhNWNiMTAxYzYwYzQ2MDVlOTQ0ZTI0MTExZDdhNTZjYmNiN2MyNTFlIn0=; _px=2aTrZ4YsWDPHKEWd8SkGSknh+MguaFO5v08Zp5FpuXihOKSBfCNW1kqz0Sh6TeY6yCxZB569T9wcqASHHLqO8w==:1000:ktdy5OvwfDY+9o/aY+fpibJo69IGspZ0rUiGyjqHr6mPvPrMzOeSncOiyE2DwR0n6XM9iFjSH45EQdDEtT7C7jywMISr/wig/QhIjCSIfjb6oN8OD5BSZIdDe9iOqI3pTIMDjAFBA8pN1OG2YClj7veSzw0gcFqQibMkTSUhXrlSWu0rtsixXA5AzmhXTic/oiIgfjYO+ArCArHmirIjyWPoomfbnd9sT/fxhC6Yha6uzK5qIw10BhAAUXUQU8oKhYP/hs+r3ZnuGOoyHPjtmg==; _pxde=f28ee823697b2a8c811f655dd2d1091c8bd93e6933895256adebef386d4e4bd0:eyJ0aW1lc3RhbXAiOjE1NzA0MDEzMzM0NTYsImZfa2IiOjB9; aasd=11%7C1570401017239'\n",
    "    dow_30_companies = get_links(dow_30_companies, cookies)\n",
    "    #dow_30_companies.to_csv('companies.txt')\n",
    "    dow_30_companies = data_companies(dow_30_companies,cookies)\n",
    "    #cookies = input(\"Enter your cookies for the rerun \")\n",
    "    dow_30_companies.to_csv('companies.csv')\n",
    "    dow_30_companies = rerun_companies(dow_30_companies, cookies)\n",
    "    dow_30_companies.to_csv('companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
